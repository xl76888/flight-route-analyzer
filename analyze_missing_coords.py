#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æç¼ºå¤±åæ ‡çš„èˆªçº¿
"""

from fix_parser import parse_excel_route_data
from data_cleaner import clean_route_data
from airport_coords import get_airport_coords
import pandas as pd
from collections import Counter

def analyze_missing_coordinates():
    """åˆ†æç¼ºå¤±åæ ‡çš„èˆªçº¿"""
    print("ğŸ” åˆ†æç¼ºå¤±åæ ‡çš„èˆªçº¿")
    print("=" * 50)
    
    # åŠ è½½å’Œæ¸…ç†æ•°æ®
    routes_df = parse_excel_route_data('data/å¤§é™†èˆªå¸å…¨è´§æœºèˆªçº¿.xlsx')
    routes_df = clean_route_data(routes_df, enable_deduplication=False)
    
    print(f"æ€»èˆªçº¿æ•°: {len(routes_df)}")
    
    # åˆ†æç¼ºå¤±åæ ‡çš„åŸå¸‚
    missing_origins = []
    missing_destinations = []
    missing_routes = []
    
    for idx, row in routes_df.iterrows():
        origin_coords = get_airport_coords(row['origin'])
        dest_coords = get_airport_coords(row['destination'])
        
        if not origin_coords:
            missing_origins.append(row['origin'])
        if not dest_coords:
            missing_destinations.append(row['destination'])
        
        if not origin_coords or not dest_coords:
            missing_routes.append({
                'airline': row['airline'],
                'origin': row['origin'],
                'destination': row['destination'],
                'missing_origin': not origin_coords,
                'missing_dest': not dest_coords
            })
    
    print(f"\nâŒ ç¼ºå¤±åæ ‡çš„èˆªçº¿æ•°: {len(missing_routes)}")
    print(f"âŒ ç¼ºå¤±èµ·ç‚¹åæ ‡çš„è®°å½•æ•°: {len(missing_origins)}")
    print(f"âŒ ç¼ºå¤±ç»ˆç‚¹åæ ‡çš„è®°å½•æ•°: {len(missing_destinations)}")
    
    # ç»Ÿè®¡ç¼ºå¤±æœ€å¤šçš„åŸå¸‚
    print("\nğŸ™ï¸ ç¼ºå¤±åæ ‡æœ€å¤šçš„èµ·ç‚¹åŸå¸‚:")
    origin_counts = Counter(missing_origins)
    for city, count in origin_counts.most_common(10):
        print(f"  {city}: {count} æ¬¡")
    
    print("\nğŸ™ï¸ ç¼ºå¤±åæ ‡æœ€å¤šçš„ç»ˆç‚¹åŸå¸‚:")
    dest_counts = Counter(missing_destinations)
    for city, count in dest_counts.most_common(10):
        print(f"  {city}: {count} æ¬¡")
    
    # æ‰¾å‡ºæ‰€æœ‰å”¯ä¸€çš„ç¼ºå¤±åŸå¸‚
    all_missing_cities = set(missing_origins + missing_destinations)
    print(f"\nğŸ“ éœ€è¦è¡¥å……åæ ‡çš„åŸå¸‚æ€»æ•°: {len(all_missing_cities)}")
    
    # æŒ‰é¢‘ç‡æ’åº
    all_missing_counts = Counter(missing_origins + missing_destinations)
    print("\nğŸ¯ æŒ‰å½±å“èˆªçº¿æ•°æ’åºçš„ç¼ºå¤±åŸå¸‚:")
    for city, count in all_missing_counts.most_common(20):
        print(f"  {city}: å½±å“ {count} æ¡èˆªçº¿")
    
    # åˆ†æè¿™äº›åŸå¸‚çš„ç‰¹å¾
    print("\nğŸ” ç¼ºå¤±åŸå¸‚åˆ†æ:")
    chinese_cities = [city for city in all_missing_cities if any('\u4e00' <= char <= '\u9fff' for char in city)]
    english_cities = [city for city in all_missing_cities if not any('\u4e00' <= char <= '\u9fff' for char in city)]
    
    print(f"  ä¸­æ–‡åŸå¸‚å: {len(chinese_cities)} ä¸ª")
    print(f"  è‹±æ–‡åŸå¸‚å: {len(english_cities)} ä¸ª")
    
    # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
    print("\nğŸ“ ä¸­æ–‡åŸå¸‚ç¤ºä¾‹:")
    for city in sorted(chinese_cities)[:10]:
        print(f"  {city}")
    
    print("\nğŸ“ è‹±æ–‡åŸå¸‚ç¤ºä¾‹:")
    for city in sorted(english_cities)[:10]:
        print(f"  {city}")
    
    # ä¿å­˜è¯¦ç»†åˆ†æç»“æœ
    missing_df = pd.DataFrame(missing_routes)
    missing_df.to_csv('data/missing_coordinates_analysis.csv', index=False, encoding='utf-8-sig')
    print(f"\nğŸ’¾ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜åˆ°: data/missing_coordinates_analysis.csv")
    
    # ç”Ÿæˆåæ ‡è¡¥å……å»ºè®®
    print("\nğŸ’¡ åæ ‡è¡¥å……å»ºè®®:")
    print("1. ä¼˜å…ˆè¡¥å……å½±å“èˆªçº¿æ•°æœ€å¤šçš„åŸå¸‚åæ ‡")
    print("2. å¯¹äºæœºåœºå…¨åï¼Œå¯ä»¥æ˜ å°„åˆ°å¯¹åº”çš„åŸå¸‚åæ ‡")
    print("3. å¯¹äºè‹±æ–‡åŸå¸‚åï¼Œéœ€è¦æŸ¥æ‰¾å¯¹åº”çš„ç»çº¬åº¦")
    print("4. è¡¥å……åé¢„è®¡å¯å¢åŠ æ˜¾ç¤ºèˆªçº¿æ•°:", len(missing_routes))
    
    return all_missing_counts

if __name__ == "__main__":
    result = analyze_missing_coordinates()